{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd #for reading csv files\n",
    "from sklearn.model_selection import GridSearchCV  # Import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training=pd.read_csv('training_data.csv', sep=',')\n",
    "test=pd.read_csv('songs_to_classify.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only use five features version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select which features to use\n",
    "features = ['danceability','key','loudness','instrumentalness','liveness']\n",
    "\n",
    "# i learn how to do one-hot encode from chatgpt because i realized there is no size meaning in the key feature.  \n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "X_train_selected = training[features]\n",
    "X_test_selected = test[features]\n",
    "# OneHotEncode the 'key' feature \n",
    "X_train_key_encoded = encoder.fit_transform(X_train_selected[['key']])\n",
    "X_test_key_encoded = encoder.transform(X_test_selected[['key']])\n",
    "\n",
    "X_train_no_key = X_train_selected.drop('key', axis=1).values\n",
    "X_test_no_key = X_test_selected.drop('key', axis=1).values\n",
    "\n",
    "X_train = np.hstack([X_train_no_key, X_train_key_encoded])\n",
    "X_test = np.hstack([X_test_no_key, X_test_key_encoded])\n",
    "\n",
    "# extract labels\n",
    "y_train = training.loc[:, 'label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data. Can also be done using sklearn methods such as\n",
    "# MinMaxScaler()\n",
    "X_trainn = X_train*1/np.max(np.abs(X_train),axis=0)\n",
    "X_testn = X_test*1/np.max(np.abs(X_train),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using all features version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i learned how to choose all the features from chatgpt but now i think i could figure it out myself\n",
    "\n",
    "#encoder = OneHotEncoder(sparse=False)\n",
    "#X_train_key_encoded = encoder.fit_transform(X_train_selected[['key']])\n",
    "#X_test_key_encoded = encoder.transform(X_test_selected[['key']])\n",
    "\n",
    "\n",
    "# Remove the 'key' column and keep other features\n",
    "#X_train_no_key_time = training.loc[:, training.columns != 'key'].drop('label', axis=1).values\n",
    "#X_test_no_key_time = test.loc[:, test.columns != 'key'].values\n",
    "\n",
    "# Merge OneHot encoded 'key' with other features\n",
    "#X_train = np.hstack([X_train_no_key_time, X_train_key_encoded, X_train_time_signature_encoded])\n",
    "#X_test = np.hstack([X_test_no_key_time, X_test_key_encoded, X_test_time_signature_encoded])\n",
    "\n",
    "# Extract labels\n",
    "#y_train = training.loc[:, 'label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: all inputs/features are treated as quantitative/numeric\n",
    "# some of the features are perhaps more sensible to treat as\n",
    "# qualitative/cathegorical. For that sklearn preprocessing methods\n",
    "# such as OneHotEncoder() can be used\n",
    "\n",
    "# Define the parameter grid for n_neighbors\n",
    "knnmodel = KNeighborsClassifier(n_neighbors = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the k-NN model. To set n_neighbors in a systematic way, use cross validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_neighbors: 7\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for n_neighbors\n",
    "\n",
    "# i use chatgpt to generate this code for choosing these hyperparameters using GridSearchCV. \n",
    "# I know this method but i was not sure how to implement it in code.\n",
    "param_grid = {'n_neighbors': range(1, 21)}  # Try n_neighbors from 1 to 20\n",
    "\n",
    "# Use GridSearchCV to find the best n_neighbors\n",
    "grid_search = GridSearchCV(estimator=knnmodel, param_grid=param_grid, cv=5)  # 5-fold cross-validation\n",
    "grid_search.fit(X=X_trainn, y=y_train)\n",
    "\n",
    "# Get the best n_neighbors from the grid search\n",
    "best_n_neighbors = grid_search.best_params_['n_neighbors']\n",
    "print(f\"Best n_neighbors: {best_n_neighbors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=7)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the k-NN model with the best n_neighbors\n",
    "knnmodel_best = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n",
    "knnmodel_best.fit(X=X_trainn, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00010001101001111011001000110110111111011001110111111101110011001101110011010110111001111110001111100010110011100101000001101110111111111110001111001111101111100100101111111010111001111100011010111111\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "# Chagpt designed this output format for me and i asked it to remove spaces between elements so that i could use it for submission\n",
    "predictions = knnmodel_best.predict(X=X_testn).reshape(-1, 1).astype(int).reshape(1, -1)\n",
    "#print(predictions)\n",
    "print(''.join(map(str, predictions.flatten())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN cross-validation accuracy: 0.7267 ± 0.0280\n"
     ]
    }
   ],
   "source": [
    "# i use chatgpt to do cross validation because I need a standard to evaluate the pros and cons of different models\n",
    "cv_scores_lr = cross_val_score(knnmodel_best, X_trainn, y_train, cv=5, scoring='accuracy')\n",
    "print(f'KNN cross-validation accuracy: {cv_scores_lr.mean():.4f} ± {cv_scores_lr.std():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
