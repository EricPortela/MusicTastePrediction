{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression  # Import LogisticRegression\n",
    "import pandas as pd #for reading csv files\n",
    "from sklearn.model_selection import GridSearchCV  # Import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training=pd.read_csv('training_data.csv', sep=',')\n",
    "test=pd.read_csv('songs_to_classify.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only use five features version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select which features to use\n",
    "features = ['danceability','key','loudness','instrumentalness','liveness']\n",
    "\n",
    "# i learn how to do one-hot encode from chatgpt because i realized there is no size meaning in the key feature.  \n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "X_train_selected = training[features]\n",
    "X_test_selected = test[features]\n",
    "# OneHotEncode the 'key' feature\n",
    "X_train_key_encoded = encoder.fit_transform(X_train_selected[['key']])\n",
    "X_test_key_encoded = encoder.transform(X_test_selected[['key']])\n",
    "\n",
    "X_train_no_key = X_train_selected.drop('key', axis=1).values\n",
    "X_test_no_key = X_test_selected.drop('key', axis=1).values\n",
    "\n",
    "X_train = np.hstack([X_train_no_key, X_train_key_encoded])\n",
    "X_test = np.hstack([X_test_no_key, X_test_key_encoded])\n",
    "\n",
    "# extract labels\n",
    "y_train = training.loc[:, 'label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "\n",
    "X_trainn = X_train * 1 / np.max(np.abs(X_train), axis=0)\n",
    "X_testn = X_test * 1 / np.max(np.abs(X_test), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using all features version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i learned how to choose all the features from chatgpt but now i think i could figure it out myself\n",
    "\n",
    "#encoder = OneHotEncoder(sparse=False)\n",
    "#X_train_key_encoded = encoder.fit_transform(X_train_selected[['key']])\n",
    "#X_test_key_encoded = encoder.transform(X_test_selected[['key']])\n",
    "#X_train_time_signature_encoded = encoder.fit_transform(training[['time_signature']])\n",
    "#X_test_time_signature_encoded = encoder.transform(test[['time_signature']])\n",
    "\n",
    "# 移除 'key' 和 'time_signature' 列，并保留其他特征\n",
    "#X_train_no_key_time = training.loc[:, (training.columns != 'key') & (training.columns != 'time_signature')].drop('label', axis=1).values\n",
    "#X_test_no_key_time = test.loc[:, (test.columns != 'key') & (test.columns != 'time_signature')].values\n",
    "\n",
    "# 合并 OneHot 编码后的 'key' 和 'time_signature' 与其他特征\n",
    "#X_train = np.hstack([X_train_no_key_time, X_train_key_encoded, X_train_time_signature_encoded])\n",
    "#X_test = np.hstack([X_test_no_key_time, X_test_key_encoded, X_test_time_signature_encoded])\n",
    "# 提取标签\n",
    "#y_train = training.loc[:, 'label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logistic regression model with L2 regularization\n",
    "logistic_model = LogisticRegression(penalty='l2', solver='lbfgs', C=1.0)  # C is the inverse of regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 10, 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "\n",
    "# i use chatgpt to generate this code for choosing these hyperparameters using GridSearchCV. \n",
    "# I know this method but i was not sure how to implement it in code.\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # 正则化强度的不同值\n",
    "    'solver': ['lbfgs', 'liblinear'],  # 不同的求解器\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=logistic_model, param_grid=param_grid, \n",
    "                           scoring='accuracy', cv=5)  # 5-fold cross-validation\n",
    "\n",
    "# Train the model using GridSearchCV\n",
    "grid_search.fit(X=X_trainn, y=y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best parameters: {best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00010001001101001011001000110010111001011101110111101001100011000111100001000110111110111010001111110010000001110011110001101111101111111101101110001011101111110101100100111001001001101100100010011111\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with the best model\n",
    "\n",
    "# Chagpt designed this output format for me and i asked it to remove spaces between elements so that i could use it for submission\n",
    "best_model = grid_search.best_estimator_\n",
    "predictions = best_model.predict(X=X_testn).reshape(-1, 1).astype(int).reshape(1, -1)\n",
    "#print(predictions)\n",
    "print(''.join(map(str, predictions.flatten())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression cross-validation accuracy: 0.7280 ± 0.0344\n"
     ]
    }
   ],
   "source": [
    "# i use chatgpt to do cross validation because I need a standard to evaluate the pros and cons of different models\n",
    "cv_scores_lr = cross_val_score(logistic_model, X_trainn, y_train, cv=5, scoring='accuracy')\n",
    "print(f'Logistic Regression cross-validation accuracy: {cv_scores_lr.mean():.4f} ± {cv_scores_lr.std():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
